{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics.nn.tasks import DetectionModel, ClassificationModel\n",
    "# from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a model\n",
    "# model = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n",
    "\n",
    "# # Run batched inference on a list of images\n",
    "# results = model([\"cycle.jpg\"])  # return a list of Results objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as FT\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from yolov1.model import Yolov1\n",
    "from yolov1.dataset import BeanDataset\n",
    "from yolov1.utils import (\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    intersection_over_union,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    upscale_boxes,\n",
    ")\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters etc.\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = (\n",
    "    1  # 64 in original paper but I don't have that much vram, grad accum?\n",
    ")\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "LOAD_MODEL_FILE = \"model.pth.tar\"\n",
    "LOAD_MODEL_FILE2 = \"vgg16.pth.tar\"\n",
    "\n",
    "TRAIN_IMG_DIR = \"/home/paineni/MasterThesis/yolov8/images/train\"\n",
    "TRAIN_LABEL_DIR = \"/home/paineni/MasterThesis/yolov8/labels/train\"\n",
    "TEST_IMG_DIR = \"/home/paineni/MasterThesis/yolov8/images/test\"\n",
    "TEST_LABEL_DIR = \"/home/paineni/MasterThesis/yolov8/labels/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paineni/micromamba/envs/Thesis-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paineni/micromamba/envs/Thesis-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m      5\u001b[0m         [\n\u001b[1;32m      6\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mparameters()},  \u001b[38;5;66;03m# parameters of YOLO model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LOAD_MODEL:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# Load YOLO model and optimizer\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOAD_MODEL_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Load VGG-16 model (no need to load optimizer again)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         load_checkpoint(LOAD_MODEL_FILE2, vgg16)\n",
      "File \u001b[0;32m~/MasterThesis/yolov1/utils.py:389\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint, model, optimizer)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(checkpoint, model, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> Loading checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 389\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimizer:\n\u001b[1;32m    391\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "model = Yolov1(split_size=7, num_boxes=2, num_classes=2).to(DEVICE)\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "optimizer = optim.Adam(\n",
    "        [\n",
    "            {\"params\": model.parameters()},  # parameters of YOLO model\n",
    "            {\"params\": vgg16.classifier.parameters()},\n",
    "        ],  # parameters of VGG-16 classifier\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "\n",
    "if LOAD_MODEL:\n",
    "        # Load YOLO model and optimizer\n",
    "        load_checkpoint(LOAD_MODEL_FILE, model, optimizer)\n",
    "\n",
    "        # Load VGG-16 model (no need to load optimizer again)\n",
    "        load_checkpoint(LOAD_MODEL_FILE2, vgg16)\n",
    "\n",
    "test_dataset = BeanDataset(\n",
    "    transform=transform,\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    label_dir=TEST_LABEL_DIR,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for z, x, y in test_loader:\n",
    "#     x = x.to(DEVICE)\n",
    "#     z = z.to(\n",
    "#         DEVICE\n",
    "#     )  # Ensure original images are also moved to the correct device\n",
    "#     for idx in range(1):  # Iterate over each sample in the batch\n",
    "#         # Get bounding boxes from the model predictions\n",
    "#         bboxes = cellboxes_to_boxes(model(x))\n",
    "#         print(f\"Number of bounding boxes before NMS: {len(bboxes)}\")\n",
    "\n",
    "#         # Apply non-max suppression\n",
    "#         bboxes2 = non_max_suppression(\n",
    "#             bboxes[idx],\n",
    "#             iou_threshold=0.5,\n",
    "#             threshold=0.4,\n",
    "#             box_format=\"midpoint\",\n",
    "#         )\n",
    "#         print(f\"Number of bounding boxes after NMS: {len(bboxes2)}\")\n",
    "\n",
    "#         # Plot the transformed image with original bounding boxes\n",
    "#         plot_image(\n",
    "#             x[idx].permute(1, 2, 0).to(\"cpu\"),\n",
    "#             bboxes2,\n",
    "#             \"/home/paineni/MasterThesis/yolov8/images/output.jpg\",\n",
    "#         )\n",
    "\n",
    "#         # Plot the original image with upscaled bounding boxes\n",
    "#         print(z[idx].permute(1, 2, 0).to(\"cpu\").shape)\n",
    "#         plot_image(\n",
    "#             z[idx].permute(1, 2, 0).to(\"cpu\"),\n",
    "#             bboxes2,\n",
    "#             \"/home/paineni/MasterThesis/yolov8/images/output2.jpg\",\n",
    "#         )\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def process_images_with_bboxes(image, predictions, output_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Processes images with bounding boxes: crops the regions and resizes them.\n",
    "\n",
    "    Args:\n",
    "    - image (PIL.Image or numpy.array): The input image.\n",
    "    - predictions (list): A list of predictions, where each prediction is a tuple\n",
    "                          (class, probability, x_center, y_center, width, height).\n",
    "    - output_size (tuple): The desired output size of the cropped images (default is (224, 224)).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of PIL.Image objects cropped and resized according to the bounding boxes.\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        raise TypeError(\"Image should be a PIL.Image object or a numpy array.\")\n",
    "\n",
    "    width, height = image.size\n",
    "    cropped_images = []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        class_label, prob, x_center, y_center, box_width, box_height = (\n",
    "            prediction\n",
    "        )\n",
    "\n",
    "        # Calculate bounding box coordinates\n",
    "        left = (x_center - box_width / 2) * width\n",
    "        top = (y_center - box_height / 2) * height\n",
    "        right = (x_center + box_width / 2) * width\n",
    "        bottom = (y_center + box_height / 2) * height\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_img = image.crop((left, top, right, bottom))\n",
    "\n",
    "        # Resize the cropped image\n",
    "        resized_img = cropped_img.resize(output_size, Image.LANCZOS)\n",
    "        resized_img_tensor = (\n",
    "            torch.from_numpy(np.array(resized_img)).permute(2, 0, 1).float()\n",
    "        )\n",
    "        cropped_images.append(resized_img_tensor)\n",
    "\n",
    "    return cropped_images, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = process_images_with_bboxes(\n",
    "    z[idx].permute(1, 2, 0).to(\"cpu\").numpy(), bboxes2, output_size=(224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cropped_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_cropped_images(cropped_images):\n",
    "    \"\"\"\n",
    "    Plots cropped images from the list of tensors.\n",
    "\n",
    "    Args:\n",
    "    - cropped_images (list): A list of torch.Tensor objects representing cropped images.\n",
    "    \"\"\"\n",
    "    num_images = len(cropped_images)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n",
    "\n",
    "    for idx, cropped_img in enumerate(cropped_images):\n",
    "        # Convert tensor to numpy array for plotting\n",
    "        img_np = cropped_img\n",
    "        axs[idx].imshow(img_np)\n",
    "        axs[idx].set_title(f\"Image {idx + 1}\")\n",
    "        axs[idx].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cropped_images(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# # Internship Timeline\n",
    "# timeline = [\n",
    "#     (\"Categorization of New Images\", 3),\n",
    "#     (\"Validating Data Quality\", 4),\n",
    "#     (\"Model Experimenting, Training and Validation\", 6),\n",
    "#     (\"Finalizing More Promising Detection Model\", 2),\n",
    "#     (\"Deploying Model in Production Env\", 2),\n",
    "#     (\"Adding CI Pipeline (Optional)\", 2),\n",
    "# ]\n",
    "\n",
    "# # Base time\n",
    "# base_time = 0.5  # weeks\n",
    "\n",
    "# # Create Gantt chart\n",
    "# fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "# fig.patch.set_facecolor('#18242c')  # Set the background color of the figure\n",
    "\n",
    "# # Set initial position for bars\n",
    "# y_positions = range(len(timeline))\n",
    "# start = 0\n",
    "\n",
    "# for i, (task, weeks) in enumerate(timeline):\n",
    "#     relative_duration = max(0, weeks - base_time) / (\n",
    "#         max([dur for _, dur in timeline]) - base_time\n",
    "#     )\n",
    "#     color = plt.cm.Oranges(relative_duration)\n",
    "#     ax.barh(\n",
    "#         y_positions[i],\n",
    "#         width=weeks,\n",
    "#         left=start,\n",
    "#         color=color,\n",
    "#         label=f\"{weeks} weeks\",\n",
    "#     )\n",
    "#     start += weeks\n",
    "\n",
    "# # Beautify the plot\n",
    "# ax.set_facecolor('#18242c')  # Set the background color of the axes\n",
    "# plt.yticks(y_positions, [task for task, _ in timeline], color='white')  # Set y-ticks text to white\n",
    "# plt.xlabel(\"Timeline (Weeks)\", color='white')  # Set x-label text to white\n",
    "# plt.title(\"PCB Defect Detection\", color='white')  # Set title text to white\n",
    "\n",
    "# # Set x-axis ticks at intervals of 2 weeks\n",
    "# ax.xaxis.set_major_locator(MultipleLocator(2))\n",
    "\n",
    "# # Change x and y axis ticks color\n",
    "# ax.tick_params(axis='x', colors='white')\n",
    "# ax.tick_params(axis='y', colors='white')\n",
    "\n",
    "# # Change the color of the axes frame to white and lines to dotted\n",
    "# ax.spines['top'].set_color('white')\n",
    "# ax.spines['right'].set_color('white')\n",
    "# ax.spines['bottom'].set_color('white')\n",
    "# ax.spines['left'].set_color('white')\n",
    "\n",
    "# # Set line style to dotted\n",
    "# ax.spines['top'].set_linestyle(':')\n",
    "# ax.spines['right'].set_linestyle(':')\n",
    "# ax.spines['bottom'].set_linestyle(':')\n",
    "# ax.spines['left'].set_linestyle(':')\n",
    "\n",
    "# # Add legend\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# legend = ax.legend(handles, labels, loc=\"lower right\", title=\"Duration\")\n",
    "# plt.setp(legend.get_title(), color='white')  # Set legend title color to white\n",
    "# plt.setp(legend.get_texts(), color='white')  # Set legend text color to white\n",
    "\n",
    "# # Set the legend background color\n",
    "# legend.get_frame().set_facecolor('#18242c')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
