{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"3xQnZJNNlVmhFwAJLzhi\")\n",
    "project = rf.workspace(\"rosenheim\").project(\"caqao-ainoc\")\n",
    "version = project.version(6)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_dir = \"cocoa-2\"\n",
    "splits = [\"train\", \"test\", \"valid\"]\n",
    "dir_labels = [os.path.join(base_dir, f\"{split}/labels\") for split in splits]\n",
    "dir_images = [os.path.join(base_dir, f\"{split}/images\") for split in splits]\n",
    "\n",
    "\n",
    "def move_files(src_dir, dest_dir):\n",
    "    if not os.path.exists(src_dir):\n",
    "        print(f\"Source directory {src_dir} does not exist. Skipping.\")\n",
    "        return\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "    for file in os.listdir(src_dir):\n",
    "        src_file = os.path.join(src_dir, file)\n",
    "        dest_file = os.path.join(dest_dir, file)\n",
    "        shutil.move(src_file, dest_file)\n",
    "\n",
    "\n",
    "# Move files from train and test to valid\n",
    "for i in range(0, 2):\n",
    "    move_files(dir_labels[i], dir_labels[2])\n",
    "    move_files(dir_images[i], dir_images[2])\n",
    "\n",
    "print(\"All files have been moved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits[:2]:  # Only train and test\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "    if os.path.exists(split_dir):\n",
    "        shutil.rmtree(split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"CAQAO-6\",\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_resolutions = set()\n",
    "\n",
    "# Loop through all the samples in the dataset\n",
    "for sample in dataset:\n",
    "    image_path = sample.filepath\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Get the resolution of the current image\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    # Add the resolution to the set\n",
    "    unique_resolutions.add((width, height))\n",
    "\n",
    "# Output all unique resolutions\n",
    "if unique_resolutions:\n",
    "    print(\"Unique resolutions found in the dataset:\")\n",
    "    for resolution in unique_resolutions:\n",
    "        print(f\"{resolution[0]}x{resolution[1]}\")\n",
    "else:\n",
    "    print(\"No images found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Load the YOLOv5 dataset\n",
    "# target_width, target_height = 4032, 3024\n",
    "\n",
    "# # Loop through all the samples in the dataset\n",
    "# for sample in dataset:\n",
    "#     image_path = sample.filepath\n",
    "#     img = cv2.imread(image_path)\n",
    "\n",
    "#     # Get the original resolution of the image\n",
    "#     original_height, original_width = img.shape[:2]\n",
    "\n",
    "#     # If the image is not already in the target resolution, resize it\n",
    "#     if (original_width, original_height) != (target_width, target_height):\n",
    "#         # Calculate the scaling factors\n",
    "#         scaling_x = target_width / original_width\n",
    "#         scaling_y = target_height / original_height\n",
    "\n",
    "#         # Resize the image to the target resolution\n",
    "#         img_resized = cv2.resize(img, (target_width, target_height))\n",
    "\n",
    "#         # Save the resized image back\n",
    "#         cv2.imwrite(image_path, img_resized)\n",
    "\n",
    "#         # Adjust bounding boxes\n",
    "#         detections = sample.ground_truth.detections  # Assuming `ground_truth` stores the detections\n",
    "#         for detection in detections:\n",
    "#             # Get the original bounding box [x, y, width, height]\n",
    "#             bbox = detection.bounding_box\n",
    "\n",
    "#             # Scale the bounding box coordinates\n",
    "#             x_min = bbox[0] * scaling_x\n",
    "#             y_min = bbox[1] * scaling_y\n",
    "#             width = bbox[2] * scaling_x\n",
    "#             height = bbox[3] * scaling_y\n",
    "\n",
    "#             # Update the bounding box with the scaled values\n",
    "#             detection.bounding_box = [x_min, y_min, width, height]\n",
    "\n",
    "#         # Save the updated sample\n",
    "#         sample.save()\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Image {image_path} is already at the target resolution.\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "samples = dataset.view()\n",
    "sample_ids = [sample.id for sample in samples]\n",
    "random.shuffle(sample_ids)\n",
    "\n",
    "# Split into train, val, test (70%, 20%, 10%)\n",
    "total_samples = len(sample_ids)\n",
    "train_split = int(0.7 * total_samples)\n",
    "val_split = int(0.9 * total_samples)\n",
    "\n",
    "train_ids = sample_ids[:train_split]\n",
    "val_ids = sample_ids[train_split:val_split]\n",
    "test_ids = sample_ids[val_split:]\n",
    "\n",
    "# Tag the samples\n",
    "for sample_id in train_ids:\n",
    "    sample = dataset[sample_id]\n",
    "    sample.tags.append(\"train\")\n",
    "    sample.save()\n",
    "\n",
    "for sample_id in val_ids:\n",
    "    sample = dataset[sample_id]\n",
    "    sample.tags.append(\"val\")\n",
    "    sample.save()\n",
    "\n",
    "for sample_id in test_ids:\n",
    "    sample = dataset[sample_id]\n",
    "    sample.tags.append(\"test\")\n",
    "    sample.save()\n",
    "\n",
    "# Export the dataset with splits\n",
    "dataset.export(\n",
    "    export_dir=\"cocoa-2-processed\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"cocoa-2-processed\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Define the class mapping\n",
    "defect_classes = [\"Slaty\", \"Mouldy\", \"Insect-damaged\", \"Germinated\"]\n",
    "non_defect_classes = [\n",
    "    \"Brown - G1\",\n",
    "    \"Partly Purple - G2\",\n",
    "    \"Partly Purple - G3\",\n",
    "    \"Total Purple - G2\",\n",
    "    \"Total Purple - G3\",\n",
    "    \"Total Purple - G4\",\n",
    "    \"Very Dark Brown - G3\",\n",
    "    \"Very Dark Brown - G4\",\n",
    "    \"Brown - G4\",\n",
    "    \"Partly Purple - G4\",\n",
    "    \"Total Purple - G1\",\n",
    "    \"Partly Purple - G1\",\n",
    "    \"Very Dark Brown - G2\",\n",
    "    \"Brown - G2\",\n",
    "    \"Very Dark Brown - G1\",\n",
    "    \"Brown - G3\",\n",
    "]\n",
    "\n",
    "class_mapping = {cls: 1 for cls in defect_classes}\n",
    "class_mapping.update({cls: 0 for cls in non_defect_classes})\n",
    "\n",
    "# Load your FiftyOne dataset\n",
    "\n",
    "# Update the dataset with the new class mapping\n",
    "for sample in ds:\n",
    "    detections = (\n",
    "        sample.ground_truth.detections\n",
    "    )  # Assuming `ground_truth` stores the detections\n",
    "    for detection in detections:\n",
    "        original_label = detection.label\n",
    "        if original_label in class_mapping:\n",
    "            detection.label = (\n",
    "                \"Defect\"\n",
    "                if class_mapping[original_label] == 1\n",
    "                else \"Non-Defect\"\n",
    "            )\n",
    "\n",
    "    # Save the updated sample\n",
    "    sample.save()\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Dataset updated with new class mappings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.export(\n",
    "    export_dir=\"cocoa-2-processed\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.Session(ds, port=27771)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in ds:\n",
    "    image_path = sample.filepath\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Resize to 1920x1080 if it doesn't match\n",
    "    if img.size != (1920, 1080):\n",
    "        print(\"no\")\n",
    "        img = img.resize((1920, 1080), Image.Resampling.LANCZOS)\n",
    "        img.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the FiftyOne dataset\n",
    "\n",
    "# Define export directory and splits\n",
    "export_dir = \"roboflow_yolov8\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Ensure the export directory exists\n",
    "Path(export_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Export each split\n",
    "for split in splits:\n",
    "    # Select samples tagged with the current split\n",
    "    split_view = ds.match_tags(split)\n",
    "\n",
    "    # Define split-specific export directory\n",
    "    split_export_dir = Path(export_dir) / split\n",
    "\n",
    "    # Convert the export directory to a string\n",
    "    split_export_dir_str = str(split_export_dir)\n",
    "\n",
    "    # Export the split\n",
    "    split_view.export(\n",
    "        export_dir=split_export_dir_str,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,  # YOLOv8 uses the same format as YOLOv5\n",
    "        label_field=\"ground_truth\",  # Replace with your label field\n",
    "        include_media=True,\n",
    "        overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the root directory\n",
    "root_dir = export_dir\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    # Define paths\n",
    "    split_dir = Path(root_dir) / split\n",
    "    images_src = split_dir / \"images\" / \"val\"\n",
    "    labels_src = split_dir / \"labels\" / \"val\"\n",
    "    req_dir = Path(root_dir)\n",
    "    images_dest = req_dir / \"images\" / split\n",
    "    labels_dest = req_dir / \"labels\" / split\n",
    "\n",
    "    # Create destination directories if not exist\n",
    "    images_dest.mkdir(parents=True, exist_ok=True)\n",
    "    labels_dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move image files\n",
    "    if images_src.exists():\n",
    "        for file in images_src.iterdir():\n",
    "            if file.is_file():\n",
    "                shutil.move(str(file), images_dest)\n",
    "        # Remove the now-empty directory\n",
    "        images_src.rmdir()\n",
    "\n",
    "    # Move label files\n",
    "    if labels_src.exists():\n",
    "        for file in labels_src.iterdir():\n",
    "            if file.is_file():\n",
    "                shutil.move(str(file), labels_dest)\n",
    "        # Remove the now-empty directory\n",
    "        labels_src.rmdir()\n",
    "\n",
    "# Verify and print the final structure\n",
    "print(\"Dataset rearranged successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def xyxy2normalized_topleftxywh(\n",
    "    xmin: Union[float, np.ndarray],\n",
    "    ymin: Union[float, np.ndarray],\n",
    "    xmax: Union[float, np.ndarray],\n",
    "    ymax: Union[float, np.ndarray],\n",
    "    bg_w: Union[int, np.ndarray],\n",
    "    bg_h: Union[int, np.ndarray],\n",
    "):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    x_topleft_norm = xmin / bg_w\n",
    "    y_topleft_norm = ymin / bg_h\n",
    "    w_norm = w / bg_w\n",
    "    h_norm = h / bg_h\n",
    "\n",
    "    return x_topleft_norm, y_topleft_norm, w_norm, h_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"cocoa-2-processed\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.default_classes = [\"Defect\", \"Non-Defect\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal yolo pred (change jupyter kernel to paper_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in ds:\n",
    "    detections = []\n",
    "    v8_result = model.predict(sample.filepath, imgsz=640)\n",
    "    img = v8_result[0].orig_img\n",
    "    preds = v8_result[0].boxes.data\n",
    "    for i, box in enumerate(preds):\n",
    "        x1, y1, x2, y2 = box[:4].int().tolist()\n",
    "        cnf = float(box[4])  # Confidence score from YOLO output\n",
    "        class_id = int(box[5])  # Class ID from YOLO output\n",
    "\n",
    "        # Convert to normalized top-left xywh format\n",
    "        x_topleft_norm, y_topleft_norm, w_norm, h_norm = (\n",
    "            xyxy2normalized_topleftxywh(x1, y1, x2, y2, 1920, 1080)\n",
    "        )\n",
    "\n",
    "        # Append to FiftyOne detections\n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=ds.default_classes[class_id],\n",
    "                bounding_box=[x_topleft_norm, y_topleft_norm, w_norm, h_norm],\n",
    "                confidence=cnf,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Assign detections to the sample and save\n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined yolo pred (change jupyter kernel to paper_env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from ultralytics.engine.trainer import get_vgg16_model\n",
    "from torchvision import transforms\n",
    "from EMA import (\n",
    "    EMA_region_begin,\n",
    "    EMA_region_define,\n",
    "    EMA_region_end,\n",
    "    EMA_init,\n",
    "    EMA_finalize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v8_model = YOLO(\n",
    "    \"/home/paineni/MasterThesis/runs/detect/train2/weights/best.pt\"\n",
    ")\n",
    "vgg_model, _ = get_vgg16_model()\n",
    "vgg_model_path = \"vgg_16_16.pth\"\n",
    "vgg_model.load_state_dict(torch.load(vgg_model_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.to(\"cuda\")\n",
    "vgg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformations\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_class_prediction(cropped_image, model):\n",
    "    image = preprocess(cropped_image)\n",
    "    image = image.unsqueeze(0).to(\n",
    "        \"cuda\"\n",
    "    )  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probabilities = F.softmax(\n",
    "            output, dim=1\n",
    "        )  # Apply softmax to get probabilities\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    return predicted.item(), confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "def resize_with_padding_pil(\n",
    "    image, output_size=(224, 224), padding_color=(0, 0, 0)\n",
    "):\n",
    "    \"\"\"\n",
    "    Resizes a PIL image to the desired size with padding to maintain the aspect ratio.\n",
    "    If the image is smaller than the target size, only padding is applied.\n",
    "\n",
    "    Args:\n",
    "    - image (PIL.Image.Image): The input image.\n",
    "    - output_size (tuple): The desired output size as (height, width). Default is (224, 224).\n",
    "    - padding_color (tuple): The RGB color for the padding. Default is black (0, 0, 0).\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: The resized and padded image.\n",
    "    \"\"\"\n",
    "    # Get the original dimensions\n",
    "    original_width, original_height = image.size\n",
    "    target_width, target_height = output_size\n",
    "\n",
    "    # Check if resizing is needed\n",
    "    if original_width > target_width or original_height > target_height:\n",
    "        # Calculate the scaling factor\n",
    "        scale = min(\n",
    "            target_width / original_width, target_height / original_height\n",
    "        )\n",
    "\n",
    "        # Compute new dimensions\n",
    "        new_width = int(original_width * scale)\n",
    "        new_height = int(original_height * scale)\n",
    "\n",
    "        # Resize the image\n",
    "        image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # Add padding to make the final size match output_size\n",
    "    padded_image = ImageOps.pad(\n",
    "        image,\n",
    "        output_size,\n",
    "        color=padding_color,\n",
    "        centering=(0.5, 0.5),  # Center the image\n",
    "    )\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMA_init()\n",
    "region = EMA_region_define(\"abc\")\n",
    "EMA_region_begin(region)\n",
    "for sample in ds:\n",
    "    detections = []\n",
    "    v8_result = v8_model.predict(\n",
    "        sample.filepath,\n",
    "        imgsz=640,\n",
    "    )\n",
    "    img = v8_result[0].orig_img\n",
    "    preds = v8_result[0].boxes.data\n",
    "    for i, box in enumerate(preds):\n",
    "        x1, y1, x2, y2 = box[:4].int().tolist()\n",
    "        cropped_np = img[y1:y2, x1:x2, :]\n",
    "        cropped_image = Image.fromarray(\n",
    "            cropped_np\n",
    "        )  # Convert to PIL Image for preprocessing\n",
    "        cropped_image = resize_with_padding_pil(\n",
    "            cropped_image, output_size=(224, 224)\n",
    "        )\n",
    "\n",
    "        class_id, cnf = get_class_prediction(cropped_image, vgg_model)\n",
    "        x_topleft_norm, y_topleft_norm, w_norm, h_norm = (\n",
    "            xyxy2normalized_topleftxywh(x1, y1, x2, y2, 1920, 1080)\n",
    "        )\n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=ds.default_classes[int(class_id)],\n",
    "                bounding_box=[x_topleft_norm, y_topleft_norm, w_norm, h_norm],\n",
    "                confidence=cnf,\n",
    "            )\n",
    "        )\n",
    "        sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "        sample.save()\n",
    "EMA_region_end(region)\n",
    "EMA_finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.Session(ds, port=23672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from matplotlib.figure import Figure\n",
    "import pandas as pd\n",
    "\n",
    "artifacts_path = \"atrifacts\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split_tag in splits:\n",
    "    view = ds.match_tags([split_tag])\n",
    "\n",
    "    # Evaluate the objects in the `predictions`\n",
    "    # field with respect to the\n",
    "    # objects in the `ground_truth` field\n",
    "    eval_key = f\"eval_predictions_{split_tag}\"\n",
    "    results = view.evaluate_detections(\n",
    "        \"predictions\",\n",
    "        gt_field=\"ground_truth\",\n",
    "        compute_mAP=True,\n",
    "        classes=ds.default_classes,\n",
    "        missing=\"background\",\n",
    "        classwise=False,\n",
    "    )\n",
    "    # whether to consider objects with different label\n",
    "    # values as always non-overlapping (True) or to compute IoUs\n",
    "    # for all objects regardless of label (False)\n",
    "\n",
    "    # the COCO mAP evaluator averages the mAP\n",
    "    # over 10 IoU thresholds from 0.5 to 0.95\n",
    "    # with a step size of 0.05 (AP@[0.5:0.05:0.95])\n",
    "    # To be found in the source of fiftyone.\n",
    "    # \"https://github.com/voxel51/fiftyone/blob/\"\n",
    "    # \"acf3a8f886505d852903e320d057057813261993/fiftyone/\"\n",
    "    # \"utils/eval/coco.py#L91\"\n",
    "    mAP = results.mAP()\n",
    "    print(f\"mAP@[0.5:0.05:0.95] {split_tag} : \" + str(mAP))\n",
    "    classwise_ap_df = pd.DataFrame(columns=[\"Label\", \"AP@[0.5:0.05:0.95]\"])\n",
    "    label_values = ds.count_values(\"ground_truth.detections.label\")\n",
    "    for label in ds.default_classes:\n",
    "        class_AP = results.mAP([label])\n",
    "        print(f\"AP@[0.5:0.05:0.95] of {split_tag} ({label}): \" + str(class_AP))\n",
    "        classwise_ap_df = classwise_ap_df._append(\n",
    "            {\n",
    "                \"Label\": label,\n",
    "                \"AP@[0.5:0.05:0.95]\": class_AP,\n",
    "                \"support\": label_values[label],\n",
    "            },\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpworkdir:\n",
    "        temp_csv_path = os.path.join(\n",
    "            tmpworkdir, f\"{split_tag}_classwise_ap_values.csv\"\n",
    "        )\n",
    "        classwise_ap_df.to_csv(temp_csv_path, index=False)\n",
    "\n",
    "    results.print_report()\n",
    "    report = results.report()\n",
    "    weighted_avg_precision = report[\"weighted avg\"][\"precision\"]\n",
    "    weighted_avg_recall = report[\"weighted avg\"][\"recall\"]\n",
    "\n",
    "    # Print some statistics about the total TP/FP/FN counts\n",
    "    # mean_tp = view.sum(f\"{eval_key}_tp\")\n",
    "    # mean_fp = view.sum(f\"{eval_key}_fp\")\n",
    "    # mean_fn = view.sum(f\"{eval_key}_fn\")\n",
    "    # print(f\"TP ({split_tag}): {mean_tp}\")\n",
    "    # print(f\"FP ({split_tag}): {mean_fp}\")\n",
    "    # print(f\"FN ({split_tag}): {mean_fn}\")\n",
    "\n",
    "    class_counts = view.count_values(\"predictions.detections.label\")\n",
    "\n",
    "    # pr_curve_path = os.path.join(artifacts_path, f\"PR_curve_{split_tag}.png\")\n",
    "    # pr_curve_plot: Figure = results.plot_pr_curves(\n",
    "    #     classes=list(class_counts.keys()),\n",
    "    #     backend=\"matplotlib\",\n",
    "    #     style=\"dark_background\",\n",
    "    # )\n",
    "    # pr_curve_plot.savefig(pr_curve_path, dpi=250)\n",
    "\n",
    "    # conf_mat_path = os.path.join(\n",
    "    #     artifacts_path, f\"confusion_matrix_{split_tag}.png\"\n",
    "    # )\n",
    "    # conf_mat_plot: Figure = results.plot_confusion_matrix(backend=\"matplotlib\")\n",
    "    # conf_mat_plot.savefig(conf_mat_path, dpi=250)\n",
    "\n",
    "ds.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
